# Hardware Optimization for 2x RTX 3090

Optimized architecture for dual-GPU setup with massive headroom.

## Your Hardware

```
System Specs:
â”œâ”€ GPU 1: RTX 3090 (24GB VRAM)
â”œâ”€ GPU 2: RTX 3090 (24GB VRAM)
â”œâ”€ Total VRAM: 48GB
â”œâ”€ RAM: 96GB
â””â”€ Total Power: INSANE
```

## The Perfect Setup

### GPU Assignment Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  OPTIMAL GPU ALLOCATION                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GPU 0 (RTX 3090 #1) - 24GB VRAM
â”œâ”€ ASR Service (Port 8001)
â”‚  â”œâ”€ faster-whisper large-v3: 3GB
â”‚  â”œâ”€ Smart Turn v3 VAD: <100MB (CPU)
â”‚  â””â”€ Available VRAM: 21GB (room for 7 more instances!)
â”‚
â””â”€ Capacity: 30+ concurrent streams, easy

GPU 1 (RTX 3090 #2) - 24GB VRAM
â”œâ”€ TTS Service (Port 8002)
â”‚  â”œâ”€ Chatterbox-streaming: 2-3GB
â”‚  â””â”€ Available VRAM: 21GB (room for 7 more instances!)
â”‚
â””â”€ Capacity: 30+ concurrent syntheses, easy

Benefits:
âœ… Zero GPU contention
âœ… Maximum performance for each service
âœ… Can scale to 100+ connections easily
âœ… Simple configuration
```

### Why This is Perfect

**Dedicated GPU per Service:**
- ASR never competes with TTS for GPU
- Each service gets full 24GB to itself
- Can run larger models if needed
- Massive headroom for scaling

**Your Capacity:**

| Metric | With Your Hardware | Notes |
|--------|-------------------|-------|
| **Current target** | 20-30 connections | âœ… Easy |
| **Max capacity** | 100+ connections | With current models |
| **ASR throughput** | 50+ concurrent streams | Per GPU |
| **TTS throughput** | 50+ concurrent syntheses | Per GPU |
| **VRAM usage** | ~6GB / 48GB (12.5%) | Tons of headroom |

## Configuration

### GPU Assignment

```yaml
# services/asr/config.yaml
model:
  device: "cuda:0"  # â† GPU 0 for ASR
  name: "large-v3"
  compute_type: "float16"  # Or int8 for even more capacity

server:
  host: "0.0.0.0"
  port: 8001
  max_connections: 50  # Can handle way more than 30!
```

```yaml
# services/tts/config.yaml
model:
  device: "cuda:1"  # â† GPU 1 for TTS

server:
  host: "0.0.0.0"
  port: 8002
  max_connections: 50  # Can handle way more than 30!
```

### Environment Variables

```bash
# Start ASR service on GPU 0
CUDA_VISIBLE_DEVICES=0 uv run python services/asr/server.py

# Start TTS service on GPU 1
CUDA_VISIBLE_DEVICES=1 uv run python services/tts/server.py
```

### Systemd Services

```ini
# /etc/systemd/system/voice-agent-asr.service
[Unit]
Description=Voice Agent ASR Service (GPU 0)
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/voice-agent/services/asr
Environment="CUDA_VISIBLE_DEVICES=0"
Environment="PATH=/home/your-user/.local/bin:$PATH"
ExecStart=/home/your-user/.local/bin/uv run python server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```ini
# /etc/systemd/system/voice-agent-tts.service
[Unit]
Description=Voice Agent TTS Service (GPU 1)
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/voice-agent/services/tts
Environment="CUDA_VISIBLE_DEVICES=1"
Environment="PATH=/home/your-user/.local/bin:$PATH"
ExecStart=/home/your-user/.local/bin/uv run python server.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

## Performance Expectations

### With Your Hardware:

```
30 Concurrent Connections (Your Target):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPU 0 (ASR):
â”œâ”€ VRAM usage: ~4GB (3GB model + 1GB inference)
â”œâ”€ Utilization: ~20-30%
â””â”€ Status: UNDERUTILIZED âœ…

GPU 1 (TTS):
â”œâ”€ VRAM usage: ~4GB (3GB model + 1GB inference)
â”œâ”€ Utilization: ~30-40%
â””â”€ Status: UNDERUTILIZED âœ…

RAM:
â”œâ”€ Queues: ~50MB
â”œâ”€ Python processes: ~2GB
â”œâ”€ Audio buffers: ~500MB
â””â”€ Total: ~3GB / 96GB (3%)

Latency:
â”œâ”€ ASR: 80-120ms (GPU 0 dedicated)
â”œâ”€ TTS: 450-500ms first chunk (GPU 1 dedicated)
â””â”€ Total pipeline: 600-800ms âœ…
```

### Scaling Potential:

```
If you need to scale beyond 30 connections:

Option 1: Increase limits (same GPUs)
â”œâ”€ ASR: 50 connections on GPU 0
â”œâ”€ TTS: 50 connections on GPU 1
â””â”€ Total: 100 concurrent users

Option 2: Run multiple instances per GPU
â”œâ”€ 2x ASR instances on GPU 0 (ports 8001, 8011)
â”œâ”€ 2x TTS instances on GPU 1 (ports 8002, 8012)
â”œâ”€ Load balance with nginx
â””â”€ Total: 200+ concurrent users

Option 3: Use both GPUs for each service
â”œâ”€ ASR: Multi-GPU with model parallelism
â”œâ”€ TTS: Multi-GPU with model parallelism
â””â”€ Total: Extreme performance (probably overkill)
```

## Memory Optimization

### Current Allocation (Conservative):

```
GPU 0 (ASR):
â”œâ”€ faster-whisper large-v3: 3GB
â”œâ”€ Inference headroom: 1GB
â”œâ”€ Multi-stream buffers: 1GB
â””â”€ Total: 5GB / 24GB (21%)

GPU 1 (TTS):
â”œâ”€ Chatterbox model: 2.5GB
â”œâ”€ Voice embeddings cache: 0.5GB
â”œâ”€ Inference headroom: 1GB
â””â”€ Total: 4GB / 24GB (17%)

System RAM:
â”œâ”€ Python + dependencies: 2GB
â”œâ”€ Audio queues: 50MB
â”œâ”€ Redis (if used): 500MB
â”œâ”€ OS overhead: 2GB
â””â”€ Total: ~5GB / 96GB (5%)

You have TONS of headroom!
```

### Aggressive Optimization (For Max Throughput):

```yaml
# Even better performance with INT8 quantization

asr:
  model: "large-v3"
  compute_type: "int8_float16"  # 2x faster, half the VRAM
  # VRAM: 1.5GB (instead of 3GB)
  # Can fit 16 instances on one GPU!

tts:
  # Chatterbox doesn't support INT8 yet
  # But still plenty of room
```

## Monitoring

### GPU Monitoring Script

```bash
# scripts/monitor_gpus.sh
#!/bin/bash

watch -n 1 'nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,utilization.memory,memory.used,memory.total --format=csv,noheader,nounits'
```

**Expected output:**
```
0, NVIDIA GeForce RTX 3090, 45, 25, 20, 4096, 24576
1, NVIDIA GeForce RTX 3090, 48, 35, 18, 3584, 24576
      â†‘                    â†‘   â†‘   â†‘      â†‘      â†‘
      GPU ID              Temp GPU% MEM%  Used  Total
```

### Health Check Endpoint

```python
# services/asr/server.py
import torch

@app.get("/health")
async def health_check():
    gpu_id = 0  # ASR on GPU 0

    return {
        "status": "healthy",
        "gpu": {
            "id": gpu_id,
            "name": torch.cuda.get_device_name(gpu_id),
            "available": torch.cuda.is_available(),
            "memory_allocated": torch.cuda.memory_allocated(gpu_id) / 1e9,  # GB
            "memory_reserved": torch.cuda.memory_reserved(gpu_id) / 1e9,
            "memory_total": torch.cuda.get_device_properties(gpu_id).total_memory / 1e9,
        },
        "active_connections": len(manager.connections),
        "queue_size": manager.queue_manager.input_queue.qsize()
    }
```

```bash
# Check ASR service
curl http://localhost:8001/health

# Check TTS service
curl http://localhost:8002/health
```

## Advanced: Multi-Instance Scaling

If you ever need to handle 100+ connections:

### nginx Load Balancing

```nginx
# /etc/nginx/nginx.conf

upstream asr_backend {
    # Run 2 ASR instances on GPU 0
    server localhost:8001;
    server localhost:8011;
}

upstream tts_backend {
    # Run 2 TTS instances on GPU 1
    server localhost:8002;
    server localhost:8012;
}

server {
    listen 80;

    location /v1/stream/asr {
        proxy_pass http://asr_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }

    location /v1/stream/tts {
        proxy_pass http://tts_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

### Running Multiple Instances

```bash
# GPU 0: 2 ASR instances
CUDA_VISIBLE_DEVICES=0 uv run python server.py --port 8001 &
CUDA_VISIBLE_DEVICES=0 uv run python server.py --port 8011 &

# GPU 1: 2 TTS instances
CUDA_VISIBLE_DEVICES=1 uv run python server.py --port 8002 &
CUDA_VISIBLE_DEVICES=1 uv run python server.py --port 8012 &

# Now you can handle 200+ connections!
```

## Benchmarks with Your Hardware

### Expected Performance:

```
Single Connection:
â”œâ”€ ASR latency: 80ms (GPU 0 dedicated)
â”œâ”€ TTS first chunk: 450ms (GPU 1 dedicated)
â””â”€ Total: 530ms âœ… Excellent!

30 Concurrent Connections:
â”œâ”€ ASR latency: 100ms (slight queue wait)
â”œâ”€ TTS first chunk: 480ms (slight queue wait)
â”œâ”€ GPU 0 utilization: 25%
â”œâ”€ GPU 1 utilization: 35%
â””â”€ Total: 580ms âœ… Still great!

50 Concurrent Connections (if needed):
â”œâ”€ ASR latency: 120ms
â”œâ”€ TTS first chunk: 520ms
â”œâ”€ GPU 0 utilization: 40%
â”œâ”€ GPU 1 utilization: 50%
â””â”€ Total: 640ms âœ… Very good!

100 Concurrent Connections (extreme):
â”œâ”€ ASR latency: 200ms
â”œâ”€ TTS first chunk: 700ms
â”œâ”€ GPU 0 utilization: 70%
â”œâ”€ GPU 1 utilization: 80%
â””â”€ Total: 900ms ğŸŸ¡ Acceptable
```

## Power & Thermal Considerations

### Power Draw:

```
RTX 3090 TDP: 350W each
â”œâ”€ Idle: ~30W per GPU
â”œâ”€ At 30% load: ~120W per GPU
â”œâ”€ At 100% load: 350W per GPU
â””â”€ Your setup (30 connections): ~240W total (both GPUs)

System total: ~400W (GPUs + CPU + RAM)
```

### Thermal:

```
Expected temps at 30 connections:
â”œâ”€ GPU 0: 45-55Â°C
â”œâ”€ GPU 1: 50-60Â°C (TTS uses more)
â””â”€ Safe range: <83Â°C

Recommendations:
â”œâ”€ Good case airflow
â”œâ”€ Monitor with nvidia-smi
â””â”€ Set fan curve if needed
```

## Cost Savings

### Your Setup vs Cloud:

```
Cloud GPU instances (AWS/GCP):
â”œâ”€ 2x A100 (40GB): ~$8/hour = $5,760/month
â”œâ”€ 2x V100 (32GB): ~$4/hour = $2,880/month
â””â”€ 2x T4 (16GB): ~$1.5/hour = $1,080/month

Your bare metal setup:
â”œâ”€ Upfront cost: Already owned
â”œâ”€ Monthly cost: ~$50 electricity
â””â”€ Savings: $1,000-5,000/month âœ…
```

## Recommendations

### For Your Hardware:

1. **âœ… DO:**
   - Dedicate GPU 0 to ASR
   - Dedicate GPU 1 to TTS
   - Start with max 30-50 connections per service
   - Monitor GPU utilization with nvidia-smi
   - Use INT8 if you need more capacity

2. **âŒ DON'T:**
   - Run both services on same GPU (unnecessary)
   - Use CPU for inference (waste of GPUs)
   - Over-allocate connections (start conservative)

3. **ğŸ”§ TUNE:**
   - Increase limits gradually based on monitoring
   - Consider INT8 quantization if scaling beyond 50
   - Add load balancing if you hit 100+ connections

### Quick Start Commands:

```bash
# Terminal 1: Start ASR on GPU 0
cd services/asr
CUDA_VISIBLE_DEVICES=0 uv run python server.py

# Terminal 2: Start TTS on GPU 1
cd services/tts
CUDA_VISIBLE_DEVICES=1 uv run python server.py

# Terminal 3: Monitor GPUs
watch -n 1 nvidia-smi
```

## Summary

**Your Hardware = BEAST MODE**

- 2x RTX 3090s = Way more than you need for 30 connections
- Can easily handle 100+ with current setup
- Can scale to 200+ with multi-instance deployment
- Tons of VRAM and RAM headroom for future growth

**Configuration:**
- GPU 0: ASR only
- GPU 1: TTS only
- Simple, clean, maximum performance

**You're in excellent shape!** ğŸš€
